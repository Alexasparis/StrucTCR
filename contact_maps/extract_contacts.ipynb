{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/Users/alexascunceparis/Desktop/BSC/immuno_project/TCRranker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from Bio import PDB\n",
    "\n",
    "\n",
    "residue_mapping = {\n",
    "    'ALA': 'A', 'ARG': 'R', 'ASN': 'N', 'ASP': 'D',\n",
    "    'CYS': 'C', 'GLU': 'E', 'GLN': 'Q', 'GLY': 'G',\n",
    "    'HIS': 'H', 'ILE': 'I', 'LEU': 'L', 'LYS': 'K',\n",
    "    'MET': 'M', 'PHE': 'F', 'PRO': 'P', 'SER': 'S',\n",
    "    'THR': 'T', 'TRP': 'W', 'TYR': 'Y', 'VAL': 'V'\n",
    "}\n",
    "\n",
    "def extract_contacts(pdb_files, chain_dict, distance=5):\n",
    "    \"\"\"\n",
    "    Extract contacting atoms between specific chains based on a distance threshold.\n",
    "    \n",
    "    Args:\n",
    "        pdb_files (list of str): List of file paths to PDB files.\n",
    "        chain_dict (dict): Dictionary containing chains for each PDB ID.\n",
    "        distance (float): Distance threshold for considering contacts.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing contacts with columns ['pdb_id', 'chain_from', 'chain_to', \n",
    "                                                                 'residue_from', 'residue_to', \n",
    "                                                                 'resid_from', 'resid_to', \n",
    "                                                                 'atom_from', 'atom_to', 'dist'].\n",
    "    \"\"\"\n",
    "    contacts = []\n",
    "    \n",
    "    for pdb_file in pdb_files:\n",
    "        pdb_id = os.path.basename(pdb_file).split('.')[0]\n",
    "        print(f\"Extracting contacts from {pdb_id}\")\n",
    "        parser = PDB.PDBParser(QUIET=True)\n",
    "        structure = parser.get_structure(pdb_id, pdb_file)\n",
    "        model = structure[0]\n",
    "\n",
    "        # Retrieve chains of interest from chain_dict\n",
    "        chains = chain_dict.get(pdb_id)\n",
    "        if not chains:\n",
    "            continue\n",
    "        \n",
    "        # Define pairs of chains to extract contacts\n",
    "        chain_pairs = [\n",
    "            (chains['tcra_chain'], chains['mhc_chain']),\n",
    "            (chains['tcrb_chain'], chains['mhc_chain']),\n",
    "            (chains['tcra_chain'], chains['peptide_chain']),\n",
    "            (chains['tcrb_chain'], chains['peptide_chain'])\n",
    "        ]\n",
    "        \n",
    "        for chain_from_id, chain_to_id in chain_pairs:\n",
    "            if chain_from_id and chain_to_id:  # Ensure both chains are defined\n",
    "                try:\n",
    "                    chain_from = model[chain_from_id]\n",
    "                    chain_to = model[chain_to_id]\n",
    "                except KeyError:\n",
    "                    print(f\"Chain not found in {pdb_id}: {chain_from_id} or {chain_to_id}\")\n",
    "                    continue\n",
    "                \n",
    "                residues_from = list(chain_from.get_residues())\n",
    "                residues_to = list(chain_to.get_residues())\n",
    "                \n",
    "                for residue_from in residues_from:\n",
    "                    for residue_to in residues_to:\n",
    "                        atoms_from = list(residue_from.get_atoms())\n",
    "                        atoms_to = list(residue_to.get_atoms())\n",
    "                        \n",
    "                        for atom_from in atoms_from:\n",
    "                            for atom_to in atoms_to:\n",
    "                                dist = np.linalg.norm(atom_from.coord - atom_to.coord)\n",
    "                                if dist <= distance:  # Threshold for contact\n",
    "                                    res_from_single = residue_mapping.get(residue_from.get_resname(), 'X')  # 'X' for unknown residues\n",
    "                                    res_to_single = residue_mapping.get(residue_to.get_resname(), 'X')  # 'X' for unknown residues\n",
    "                                    \n",
    "                                    # Original contact\n",
    "                                    contacts.append([\n",
    "                                        pdb_id, \n",
    "                                        chain_from.get_id(), \n",
    "                                        chain_to.get_id(), \n",
    "                                        res_from_single, \n",
    "                                        res_to_single, \n",
    "                                        residue_from.get_id()[1], \n",
    "                                        residue_to.get_id()[1], \n",
    "                                        atom_from.get_id(), \n",
    "                                        atom_to.get_id(), \n",
    "                                        dist\n",
    "                                    ])\n",
    "    \n",
    "    return pd.DataFrame(contacts, columns=['pdb_id', 'chain_from', 'chain_to', 'residue_from', 'residue_to', 'resid_from', 'resid_to', 'atom_from', 'atom_to', 'dist'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m pdb_files \u001b[38;5;241m=\u001b[39m [os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(pdb_dir, f) \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(pdb_dir) \u001b[38;5;28;01mif\u001b[39;00m f\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.pdb\u001b[39m\u001b[38;5;124m'\u001b[39m)]\n\u001b[1;32m      4\u001b[0m chain_dict \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m----> 5\u001b[0m general_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./structures_annotation/general.txt\u001b[39m\u001b[38;5;124m'\u001b[39m, sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m pdb_id, group \u001b[38;5;129;01min\u001b[39;00m general_df\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpdb.id\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m      8\u001b[0m     chains \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      9\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtcra_chain\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     10\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtcrb_chain\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     11\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpeptide_chain\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     12\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmhc_chain\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     }\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "pdb_dir = './pdb_files/'  \n",
    "pdb_files = [os.path.join(pdb_dir, f) for f in os.listdir(pdb_dir) if f.endswith('.pdb')]\n",
    "\n",
    "chain_dict = {}\n",
    "general_df = pd.read_csv('./structures_annotation/general.txt', sep='\\t')\n",
    "\n",
    "for pdb_id, group in general_df.groupby('pdb.id'):\n",
    "    chains = {\n",
    "        'tcra_chain': None,\n",
    "        'tcrb_chain': None,\n",
    "        'peptide_chain': None,\n",
    "        'mhc_chain': None\n",
    "    }\n",
    "    \n",
    "    for _, row in group.iterrows():\n",
    "        if row['chain.component'] == 'TCR' and row['chain.type'] == 'TRA':\n",
    "            chains['tcra_chain'] = row['chain.id']\n",
    "        elif row['chain.component'] == 'TCR' and row['chain.type'] == 'TRB':\n",
    "            chains['tcrb_chain'] = row['chain.id']\n",
    "        elif row['chain.component'] == 'PEPTIDE':\n",
    "            chains['peptide_chain'] = row['chain.id']\n",
    "        elif row['chain.component'] == 'MHC' and row['chain.supertype'] == 'MHCI' and row['chain.type'] == 'MHCa':\n",
    "            chains['mhc_chain'] = row['chain.id']\n",
    "        \n",
    "    chain_dict[pdb_id] = chains \n",
    "\n",
    "\n",
    "for pdb_file in pdb_files:\n",
    "    pdb_id = os.path.basename(pdb_file).split('.')[0]\n",
    "    output_file = f'./contact_maps/{pdb_id}_contacts.csv'\n",
    "    if os.path.exists(output_file):\n",
    "        print(f\"File {output_file} exists, omiting...\")\n",
    "        continue\n",
    "    else:\n",
    "        contacts_df = extract_contacts([pdb_file], chain_dict)\n",
    "    contacts_df.to_csv(output_file, index=False)\n",
    "    print(f\"Saved contacts in {output_file}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SEE IF ALL CONTACT MAPS CAN BE USED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo filtered_3vxu_contacts.csv es válido.\n",
      "Archivo filtered_6p64_contacts.csv es válido.\n",
      "Archivo filtered_7byd_contacts.csv es válido.\n",
      "Archivo filtered_6amu_contacts.csv es válido.\n",
      "Archivo filtered_8dnt_contacts.csv es válido.\n",
      "Archivo filtered_3kps_contacts.csv es válido.\n",
      "Archivo filtered_4g9f_contacts.csv es válido.\n",
      "Archivo filtered_8qfy_contacts.csv es válido.\n",
      "Archivo filtered_8d5q_contacts.csv es válido.\n",
      "Archivo filtered_7n6e_contacts.csv es válido.\n",
      "Archivo filtered_5sws_contacts.csv es válido.\n",
      "Archivo filtered_3sjv_contacts.csv es válido.\n",
      "Archivo filtered_5nmg_contacts.csv es válido.\n",
      "Archivo filtered_7n1e_contacts.csv es válido.\n",
      "Archivo filtered_8enh_contacts.csv es válido.\n",
      "Archivo filtered_5jzi_contacts.csv es válido.\n",
      "Archivo filtered_5men_contacts.csv es válido.\n",
      "Archivo filtered_5eu6_contacts.csv es válido.\n",
      "Archivo filtered_6bj3_contacts.csv es válido.\n",
      "Archivo filtered_7jwj_contacts.csv es válido.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def validate_chain_columns(csv_file):\n",
    "    \"\"\"\n",
    "    Verifica que las columnas 'chain_from' y 'chain_to' del archivo CSV contengan exactamente 2 cadenas únicas cada una.\n",
    "\n",
    "    Args:\n",
    "    - csv_file (str): Ruta al archivo CSV.\n",
    "\n",
    "    Returns:\n",
    "    - bool: True si cumple con la condición, False si no.\n",
    "    - dict: Diccionario con las cadenas encontradas en 'chain_from' y 'chain_to'.\n",
    "    \"\"\"\n",
    "    # Cargar el archivo CSV\n",
    "    df = pd.read_csv(csv_file)\n",
    "\n",
    "    # Verificar si las columnas 'chain_from' y 'chain_to' existen\n",
    "    if 'chain_from' not in df.columns or 'chain_to' not in df.columns:\n",
    "        raise ValueError(f\"Archivo {csv_file} no contiene las columnas 'chain_from' o 'chain_to'.\")\n",
    "\n",
    "    # Obtener los valores únicos en las columnas 'chain_from' y 'chain_to'\n",
    "    unique_chain_from = df['chain_from'].unique()\n",
    "    unique_chain_to = df['chain_to'].unique()\n",
    "\n",
    "    # Validar que ambas columnas tengan exactamente 2 cadenas únicas\n",
    "    is_valid = len(unique_chain_from) == 2 and len(unique_chain_to) == 2\n",
    "\n",
    "    # Devolver el resultado y las cadenas encontradas\n",
    "    return is_valid, {'chain_from': unique_chain_from, 'chain_to': unique_chain_to}\n",
    "\n",
    "def check_contact_maps(directory):\n",
    "    \"\"\"\n",
    "    Recorre la carpeta contact_maps y verifica cada archivo CSV para asegurar que 'chain_from' y 'chain_to' tengan 2 cadenas únicas.\n",
    "\n",
    "    Args:\n",
    "    - directory (str): Directorio que contiene los archivos CSV.\n",
    "    \"\"\"\n",
    "    # Verificar que el directorio existe\n",
    "    if not os.path.isdir(directory):\n",
    "        raise ValueError(f\"El directorio {directory} no existe.\")\n",
    "\n",
    "    # Listar todos los archivos en el directorio\n",
    "    for file_name in os.listdir(directory):\n",
    "        if file_name.endswith('_contacts.csv'):\n",
    "            file_path = os.path.join(directory, file_name)\n",
    "            try:\n",
    "                is_valid, chains = validate_chain_columns(file_path)\n",
    "                if is_valid:\n",
    "                    print(f\"Archivo {file_name} es válido.\")\n",
    "                else:\n",
    "                    print(f\"Archivo {file_name} no es válido. Cadenas encontradas: {chains}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error procesando {file_name}: {e}\")\n",
    "\n",
    "check_contact_maps('./contact_maps_testing/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FILTER X RESIDUES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Carpeta donde están los archivos CSV\n",
    "folder_path = 'contact_maps'\n",
    "\n",
    "# Iterar sobre todos los archivos en la carpeta\n",
    "for filename in os.listdir(folder_path):\n",
    "    # Verificar si el archivo es un CSV\n",
    "    if filename.endswith('.csv'):\n",
    "        # Ruta completa al archivo\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        \n",
    "        # Cargar el archivo CSV\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        # Filtrar las filas donde 'residue_from' o 'residue_to' sean 'X'\n",
    "        filtered_df = df[(df['residue_from'] != 'X') & (df['residue_to'] != 'X')]\n",
    "\n",
    "        # Guardar el nuevo DataFrame en un nuevo archivo CSV\n",
    "        filtered_file_path = os.path.join(folder_path, f'filtered_{filename}')\n",
    "        filtered_df.to_csv(filtered_file_path, index=False)\n",
    "\n",
    "        print(f'Filtrado {filename} y guardado como {filtered_file_path}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anarci",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
